import re, time , sys, math, csv, os, pickle, copy
import multiprocessing as Pool
import warnings

from PyQt5.QtWidgets import QListWidget
from PyQt5.QtCore import pyqtSignal

import numpy as np
import matplotlib.pyplot as plt
from matplotlib import style
style.use("ggplot")

from sklearn import svm
from sklearn.metrics import confusion_matrix, roc_curve, auc, classification_report, accuracy_score
from sklearn.preprocessing import label_binarize
from sklearn.model_selection import LeaveOneOut, KFold, cross_val_score, train_test_split

from Utility import tfidf, Particle

from nltk.tokenize import TweetTokenizer
from nltk.corpus import stopwords

from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
from sklearn.feature_extraction.text import TfidfVectorizer

positive = 'positive'
negative = 'negative'
neutral = 'neutral'

stopwords = None

clf = None

class TrainingData:
    update = pyqtSignal(str)

    svm_standard = svm.SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
            decision_function_shape="ovr", degree=3, gamma='auto', kernel='linear',
            max_iter=-1, probability=False, random_state=None, shrinking=False,
            tol=0.001, verbose=False)

    def __init__(self, tweet, stopword, qlistwidget = None):
       self.st = open(stopword, 'r')
       self.stopWords = self.getStopWordList(stopword)

       #Read the tweets one by one and process it
       ##inpTweets = csv.reader(open('data/sampleTweets.csv', 'rb'), delimiter=',', quotechar='|')

       csvfile = open(tweet, "r")
       self.inpTweets = csv.reader(csvfile)

       self.list = qlistwidget

       warnings.filterwarnings("ignore", category=DeprecationWarning)

    #start getStopWordList
    def getStopWordList(self,stopWordListFileName):
        #read the stopwords file and build a list
        stopWords = []
        stopWords.append('AT_USER')
        stopWords.append('URL')
        stopWords.append('rt')
        stopWords.append('https')

        fp = open(stopWordListFileName, 'r')
        line = fp.readline()
        while line:
            word = line.strip()
            stopWords.append(word)
            line = fp.readline()
        fp.close()
        return stopWords
    #end

    #memulai filter tweet
    def processTweet(self,tweet):
        # process the tweets

        # menganti kata kapital menjadi kecil
        tweet = tweet.lower()
    
        # menhapus kata https://* dan www.*
        tweet = re.sub('((www\.[^\s]+)|(https?://[^\s]+))','',tweet)

        # meghapus @usename
        tweet = re.sub('@[^\s]+','',tweet)

        # menghilangkan spasi
        tweet = re.sub('[\s]+', ' ', tweet)

        # menghilangkan hastag
        #tweet = re.sub(r'#([^\s]+)', r'\1', tweet)
        tweet = re.sub(r'#([^\s]+)', '', tweet)

        tweet = re.sub(r'\\u[^\s]+', '',tweet)

        # potong
        tweet = tweet.strip('\'"')

        # menghilangkan kata kata unicode untuk emoticon
        #tweet = tweet.encode('ascii', 'ignore').decode('unicode_escape')

        tweet =  re.sub(r'\w+:\/{2}[\d\w-]+(\.[\d\w-]+)*(?:(?:\/[^\s/]*))*', '', tweet)

        myre = re.compile(u'['
                    u'\U0001F300-\U0001F5FF'
                    u'\U0001F600-\U0001F64F'
                    u'\U0001F680-\U0001F6FF'
                    u'\u2600-\u26FF\u2700-\u27BF]+', 
                    re.UNICODE)

        # menghilangkan unicode
        tweet = myre.sub('', tweet)

        # menghilangkan username
        #tweet = ' '.join(re.sub("(@[A-Za-z0-9]+)|([^0-9A-Za-z \t])|(\w+:\/\/\S+)"," ",tweet).split())
    
        return tweet
    #akhir

    #emlalkukan preprocessing t4erhadap data yang digunakan
    def preprocessingData(self,tweet, stopwords):
        self.cetak("Tweet bersih : " + tweet)

        # mempersiapkan varible pembantu
        featureList = []
        tmpFeature = []

        # mempersiapkan api untuk menjadalankan preprocessing dari tokenizer dan steamming
        nltktokenizer = TweetTokenizer()
        factorysteammer = StemmerFactory()
        stemmer = factorysteammer.create_stemmer()

        #proses tokenizer
        self.cetak("Hasil Tokenization :")
        featureList = nltktokenizer.tokenize(tweet)
        featureList = list(set(featureList))
        featureList = ' '.join(featureList).split()
        self.cetak(str(featureList))

        # menghilangkan kata stopwords

        #'''
        self.cetak("Hasil filter Stopwords :")
        for w in featureList:
            if w not in stopwords:
                tmpFeature.append(w)
    
        featureList = list(set(tmpFeature))
        featureList = ' '.join(featureList).split()
        self.cetak(str(featureList))
        tmpFeature = []

        #'''

        # proses steamming perkata agar mendapatkan kata baku

        #'''
        self.cetak("Hasil Stemming perkata:")
        for w in featureList:
            tmpFeature.append(stemmer.stem(w))

        featureList = list(set(tmpFeature))
        featureList = ' '.join(featureList).split()
        featureList = [feature for feature in featureList if not feature.isdigit()]
        self.cetak(str(featureList))
        tmpFeature = []

        self.cetak("")
        #'''

        return featureList

    #menyimpan mode4l presintence dari svm untuk dilakukan testing di website
    def exportModel(self,svm,  filename, featurelist):
        with open('featurelist.txt', 'a') as f:
            self.cetak("Mengeluarkan perhitungan dalam bentuk PKL :")
            pickle.dump(svm, open(filename, 'wb'))
            self.cetak(''.join(['Selesai training model disimpan dalam ', os.path.dirname(__file__) ,'\\', filename]))
            
            self.cetak("Mengeluarkan Feature List dalam bentuk TXT :")

            for i, feature in enumerate(featurelist):
                for a in featurelist[feature]:
                    #print(a)
                    f.write(''.join([a,",", feature]))
                    f.write("\n")

            self.cetak(''.join(['Selesai Feature List disimpan dalam ', os.path.dirname(__file__) , '\\','featurelist.txt']))                


    #function untuk menjalankan pso
    def runpso(self, trainingweight, traininglabel, svm, testingweight, testinglabel):
        swarm = {
            positive : [], 
            negative : [],
            neutral : []
            }

        temp = []

        weighthasilpso = trainingweight

        pso_tertinggi = 0
        pso_weight = trainingweight

        #preparing data for pso particle, append in one variable particle pso
        for index, value in enumerate(trainingweight):
            indexClass = [i for i, no in enumerate(svm.classes_) if no == traininglabel[index]]

            #print(self.svm_sebelumpso.coef_[indexClass])

            partic = Particle.Particle(
                trainingweight[index], 
                index, 
                svm, 
                [i for i, no in enumerate(svm.classes_) if no == traininglabel[index]],
                svm.coef_[indexClass][0]
            )

            swarm[traininglabel[index]].append(partic)

            temp.append(partic)

        hasilkedua = self.pso(swarm, trainingweight, svm)
        
        svmKedua = copy.deepcopy(self.svm_standard)
        svmKedua.fit(hasilkedua, traininglabel)

        scorekedua = svmKedua.score(trainingweight, traininglabel)

        pso_weight = hasilkedua     
            

        return weighthasilpso, pso_weight

    def pso(self, swarm, particles, svm):
        #do pso literasi 
        temp = copy.deepcopy(particles)
        svm_decision_matrix = svm.decision_function(particles)

        #print(svm_decision_matrix)

        # perubahan particle berdasarkan feature mereka
        for indexswarm, valueswarm in enumerate(swarm):
            #persiapan variable yang digunakan untuk keperluan pso
            swarm[valueswarm] = sorted(swarm[valueswarm])
            indexvalue = [i for i, no in enumerate(svm.classes_) if no == valueswarm]
            weight = svm.class_weight_[indexvalue][0]

            #print("swarm : \n", swarm[valueswarm])

            gbest = None

            globalBest = []

            #penentuan gbest bedasarkan hasil dari akumulasi persamaan svm
            for indexpbest, particlepbest in enumerate(swarm[valueswarm]):
                pbest = svm.decision_function(particlepbest.position)[0][indexvalue]
                
                '''
                print("particle position    : ", particlepbest.position)
                print("decision function    : ", svm.decision_function(particlepbest.position))
                print("pbest                : ", pbest)
                '''

                if (gbest == None) :
                    gbest = pbest
                    globalBest = swarm[valueswarm][indexpbest]  
                    self.solution = globalBest
                elif (pbest >= gbest):
                    gbest = pbest
                    globalBest = swarm[valueswarm][indexpbest]  
                    self.solution = globalBest

            #perubahan pada particle swarm
            for particle in swarm[valueswarm]:
                particle.updateParticle(globalBest.getPositionList(), weight)
                
            swarm[valueswarm] = sorted(swarm[valueswarm])
            
            #Print swarm
            for particle in swarm[valueswarm]:     
                temp[particle.index] = particle.position

        return temp

    # run program pemanggilan data
    def run(self):
        testingpersen = 0.10

        self.daftarTweet = list(self.inpTweets)

        daftar_tweet = [row[1] for row in self.daftarTweet]
        daftar_label = [row[0] for row in self.daftarTweet]

        self.cetak(str(daftar_tweet))
        self.cetak(str(daftar_label))

        self.cetak("")
        self.cetak("-------------- mempersiapkan variable ------------------")
        self.cetak("")

        global positive
        global negative
        global neutral

        featurelist = {
                    positive : [], 
                    negative : [],
                    neutral : []
                    }        
        featurelist_test = {
                    positive : [], 
                    negative : [],
                    neutral : []
                    }
        document = {
                    positive : [], 
                    negative : [],
                    neutral : []
                    }

        pso_particle = {
                    positive : [], 
                    negative : [],
                    neutral : []
                    }
        x2 = []

        tweet_labels = []

        self.cetak("-------------- Persiapan Telah Selesai ------------------")
        self.cetak("")
        self.cetak("-------------- preprocessing ------------------")
        self.cetak("")

        tweet_data_training, tweet_data_test, tweet_label_training, tweet_label_test = train_test_split(daftar_tweet, daftar_label, test_size = testingpersen, random_state = 0)

        tfidf_perhitungan, featurelist = self.preprocessing(tweet_data_training, tweet_label_training)
        tfidf_perhitungan_test, featurelist_test = self.preprocessing(tweet_data_test, tweet_label_test)

        print(featurelist)

        '''

        perhitungan tf idf untuk training

        '''

        svm_bobot = self.perhitungantfidf(tfidf_perhitungan, featurelist)
        svm_bobot_test = self.perhitungantfidf(tfidf_perhitungan_test, featurelist)
        

        self.cetak("")
        self.cetak("-------------- perhitungan SVM ------------------")
        self.cetak("")

        tfidf_len = len(svm_bobot)

        svm_bobot_split = []
        tweet_labels_split = []

        svm_bobot_fold = self.split(svm_bobot, round(len(svm_bobot) / 10))
        svm_label_fold = self.split(tweet_label_training, round(len(tweet_label_training) / 10))
        
        #for i in range(len(svm_bobot_fold)):
        for i in range(len(svm_bobot_fold)):
            svm_bobot_split.extend(svm_bobot_fold[i])
            tweet_labels_split.extend(svm_label_fold[i])

        svm_sebelumpso = copy.deepcopy(self.svm_standard)
        svm_sebelumpso.fit(svm_bobot_split, tweet_labels_split)

        if len(tweet_label_test) > 0:
            self.cetak("Hasil score testing terhadap data testing sebelum PSO: ")
            self.cetak(svm_sebelumpso.score(svm_bobot_test, tweet_label_test) * 100)
            self.cetak("")

        '''

        print hasil parameter

        '''
        
        self.cetak("")
        self.cetak("-------------- Cross Validation SVM ------------------")
        self.cetak("")
        self.printEvaluasi1(svm_sebelumpso, svm_bobot_split, tweet_labels_split)

        if len(tweet_label_test) > 0:
            self.cetak("")
            self.cetak("-------------- Cross Validation Testing Prediksi SVM  ------------------")
            self.cetak("")
            self.printEvaluasi1(svm_sebelumpso, svm_bobot_test, tweet_label_test)

        self.cetak("")
        self.cetak("-------------- Perhitungan optimasi dengan Particle Swarm Optimization ------------------")
        self.cetak("")

        svm_pso_tertinggi = svm_sebelumpso

        #pso_bobot = self.runpso(svm_bobot_split, tweet_labels_split, svm_sebelumpso, svm_bobot_test, tweet_label_test)
        while True:
            pso_bobot, pso_tertinggi = self.runpso(svm_bobot_split, tweet_labels_split, svm_sebelumpso, svm_bobot_split, tweet_labels_split)

            svm_pso_tertinggi = copy.deepcopy(self.svm_standard)
            svm_pso_tertinggi.fit(pso_tertinggi, tweet_labels_split)

            pso_score = accuracy_score(svm_pso_tertinggi.predict(pso_tertinggi), tweet_labels_split)

            if  pso_score > svm_sebelumpso.score(svm_bobot_split, tweet_labels_split):
                break

            if pso_score >= 1:
                break

        svm_pso_tertinggi = copy.deepcopy(self.svm_standard)
        svm_pso_tertinggi.fit(pso_tertinggi, tweet_labels_split)

        if len(tweet_label_test) > 0:
            self.cetak("Hasil score testing terhadap data testing setelah PSO : ")
            self.cetak(round(svm_pso_tertinggi.score(svm_bobot_test, tweet_label_test), 4) * 100)
            self.cetak("")

        '''
        
        pritn hasil evaluasi dari data

        '''

        self.cetak("")
        self.cetak("-------------- Cross Validation PSO ------------------")
        self.cetak("")
        self.printEvaluasi1(svm_pso_tertinggi, pso_tertinggi, tweet_labels_split)

        if len(tweet_label_test) > 0:
            self.cetak("")
            self.cetak("-------------- Cross Validation Testing Prediksi PSO ------------------")
            self.cetak("")
            self.printEvaluasi1(svm_pso_tertinggi, svm_bobot_test, tweet_label_test)

        self.cetak("")
        self.cetak("-------------- selesai training data ------------------")
        self.cetak("")
        self.cetak("-------------- langkah export data ------------------")
        self.cetak("")

        self.cetak("Banyaknya data  :")
        self.cetak("Data Training   :")
        self.cetak(len(tweet_data_training))
        self.cetak("Data Testing    :")
        self.cetak(len(tweet_data_test))

        featuretest = copy.deepcopy(featurelist_test)

        if len(tweet_label_test) > 0:
            for i, feature in enumerate(featurelist):
                featuretest[feature].extend(featurelist[feature])

                featuretest[feature] = list(set(featuretest[feature]))

                for entities in featurelist[feature]:
                    featuretest[feature].remove(entities)

                self.cetak("".join(["Banyaknya feature training ", str(feature)]))
                self.cetak(len(featurelist[feature]))
                self.cetak("".join(["Banyaknya feature testing ", str(feature)]))
                self.cetak(len(featurelist_test[feature]))
                self.cetak("".join(["Banyaknya selisih feature ", str(feature)]))
                self.cetak(len(featuretest[feature]))
                print(featuretest[feature])

        self.exportModel(svm_pso_tertinggi, 'modelterbaru.pkl', featurelist)


    def preprocessing(self, tweet, label):
        tweets = []

        tfidf_perhitungan = tfidf.TfIdf()

        document = {
                    positive : [], 
                    negative : [],
                    neutral : []
                    }
        
        featurelist = {
                    positive : [], 
                    negative : [],
                    neutral : []
                    }

        tweet_labels = []

        # start loop
        for i, row in enumerate(tweet):
            preprocessing_sentiment = label[i].replace('|', '')
            preprocessing_tweet = tweet[i].replace('|', '')

            self.cetak(''.join(["Preprocessing data ke ", str(i + 1)," tweet : ", preprocessing_tweet]))

            #tahap preprocessing
            preprocessing_processedtweet = self.processTweet(preprocessing_tweet)
            preprocessing_featureVector = self.preprocessingData(preprocessing_processedtweet, self.stopWords)
            tweets.append((preprocessing_featureVector, preprocessing_sentiment))

             # tahap binary
            tfidf_perhitungan.add_document(i, preprocessing_featureVector)

            if (preprocessing_sentiment == positive ):
                document[positive].append(preprocessing_tweet)

                for feature in preprocessing_featureVector:
                    featurelist[positive].append(feature)

                featurelist[positive] = list(set(featurelist[positive]))

                tweet_labels.append(positive)

            if (preprocessing_sentiment == negative ):
                document[negative].append(preprocessing_tweet)

                for feature in preprocessing_featureVector:
                   featurelist[negative].append(feature)

                featurelist[negative] = list(set(featurelist[negative]))

                tweet_labels.append(negative)

            if (preprocessing_sentiment == neutral ):
                document[neutral].append(preprocessing_tweet)

                for feature in preprocessing_featureVector:
                   featurelist[neutral].append(feature)

                featurelist[neutral] = list(set(featurelist[neutral]))

                tweet_labels.append(neutral)

        for i, data in enumerate(featurelist):
            featurelist[data] = list(set(featurelist[data]))

        return tweets, featurelist

    def perhitungantfidf(self, tfidf_perhitungan, featurelist):    
        tfidf_result = {
                    positive : [], 
                    negative : [],
                    neutral : []
                    }

        tfidf_weight = {
                    positive : [], 
                    negative : [],
                    neutral : []
                    }

        svm_bobot = []

        tfidfwkwk = tfidf.TfIdf()

        tfidfwkwk.add_document(positive, featurelist[positive])
        tfidfwkwk.add_document(negative, featurelist[negative])
        tfidfwkwk.add_document(neutral, featurelist[neutral])


        for i, feature in enumerate(tfidf_perhitungan):
            print(feature)
            b = tfidfwkwk.similarities(feature[0])
            self.cetak(b)
            a = [b[0][1],b[1][1],b[2][1]]
            svm_bobot.append(a)


        # mendapatkan pembobotan menggunakan tf idf
        '''
        for i, feature in enumerate(featurelist):
            #self.cetak("generating tf idf per feature : ".join(feature))
            tfidf_result[feature] = tfidf_perhitungan.similarities(featurelist[feature])
            
            for x in tfidf_result[feature]:
                tfidf_weight[feature].append(x[1])

        
        self.cetak("")
        self.cetak("-------------- perhitungan TF IDF ------------------")
        self.cetak("")
     
        # merubah ke variable yang bisa diterima oleh svm
        for i, row in enumerate(tfidf_weight[positive]):
            a = [tfidf_result[positive][i][1], tfidf_result[negative][i][1], tfidf_result[neutral][i][1]]
            svm_bobot.append(a)
            self.cetak("Hasil TF-IDF Tweet Ke - " + str(i + 1))
            self.cetak("Hasil TF-IDF : " + str(a))
        '''

        return svm_bobot

        #print(self.svm_bobot)
    def cetak(self, pesan):
        print(pesan)
        self.update.emit(str(pesan))

    def split(self, arr, size):
        arrs = []
        while len(arr) > size:
            pice = arr[:size]
            arrs.append(pice)
            arr   = arr[size:]
        arrs.append(arr)
        return arrs

    def printEvaluasi(self, svm, weight_fold, label_fold):
        weight = []
        label = []

        for fold in range(len(weight_fold)):
          weight.extend(weight_fold[fold])
          label.extend(label_fold[fold])
        
        self.printEvaluasi1(svm, weight, label)


    def printEvaluasi1(self, svm, weight, label):
        svm_decision_function = svm.decision_function(weight)
        svm_hasil_prediksi = svm.predict(weight)

        #print("hasil perhitungan svm :")
        #print(svm_decision_function)

        self.cetak("Bobot : ")
        self.cetak(svm.coef_)
        self.cetak("Intercept :")
        self.cetak(svm.intercept_)

        cm_akurasi_score = accuracy_score(label, svm_hasil_prediksi)
        cm_confusionmatrix = confusion_matrix(label, svm_hasil_prediksi)
        cm_classification_report = classification_report(label, svm_hasil_prediksi)

        self.cetak("Perhitungan Akurasi Training :")
        self.cetak(round(cm_akurasi_score, 4) * 100)
        self.cetak("Confusion Matrix :")
        self.cetak(cm_confusionmatrix)
        self.cetak("Laporan Klasifikasi :")
        self.cetak(cm_classification_report)

         #'''

  
        if len(weight) > 10 :
            X = np.array(weight)
            y = np.array(label)
            kf = KFold(n_splits=10)
            i = 1

            for train_index, test_index in kf.split(X):
               self.cetak("".join(["Perhitungan akurasi Fold ", str(i) ," : "]))

               X_train, X_test = X[train_index], X[test_index]
               y_train, y_test = y[train_index], y[test_index]

               svm = copy.deepcopy(self.svm_standard)
               svm.fit(X_train, y_train)
               self.cetak(round(svm.score(X_test, y_test), 4)* 100)

               fold_predik = svm.predict(X_test)
               self.cetak(confusion_matrix(fold_predik, y_test))
               self.cetak(classification_report(fold_predik, y_test))

               i = i + 1

            cross_hasil = cross_val_score(copy.deepcopy(self.svm_standard), weight, label, cv= 10, n_jobs = -1)
            self.cetak("perhitungan akurasi terhadap keseluruhan data : ")
            self.cetak(round(cross_hasil.mean(), 4) * 100)

        #'''

        '''

        pembuatan kurva roc

        '''

        '''
        
        roc_fpr = dict()
        roc_tpr = dict()
        roc_auc = dict()

        label_binari = label_binarize(svm_hasil_prediksi, svm.classes_)

        for i in range(3):
            roc_fpr[i], roc_tpr[i], _ = roc_curve(label_binari[:, i], svm_decision_function [:, i])
            roc_auc[i] = auc(roc_fpr[i], roc_tpr[i])

        #self.cetak(''.join(str(e) + " " for e in svm.predict(weight)))

        self.cetak("perhitungan akurasi terhadap keseluruhan data : ")
        self.cetak(str(round(cm_akurasi_score * 100, 2)))
        '''

        '''

        perhitungan akurasi menggunakan K-10 fold cross validation

        '''

        '''
        plt.figure()
        lw = 2
        plt.plot(fpr[2], tpr[2], color='darkorange',
                 lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[2])
        plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title('Receiver operating characteristic example')
        plt.legend(loc="lower right")
        plt.show()

        print(hasilprediksebelumpso)
        print(self.svm_sebelumpso.support_ )
        print(self.svm_sebelumpso.score(self.svm_bobot, tweet_labels))
        print(self.svm_sebelumpso.classes_)
        print(self.svm_sebelumpso.support_vectors_)
        print("cofisien for svm :")
        print(self.svm_sebelumpso.coef_)
        print("dual cofisien for svm :")
        print(self.svmsebelumpso.dual_coef_)
        '''